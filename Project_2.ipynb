{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN5r49VYNBiW1aHUCgySS/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishorgo26/Project_2/blob/main/Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jb5nZaC8gyY",
        "outputId": "9184e7a2-dbe5-4962-daa0-050f9aec06f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Artifacts â†’ /content/artifacts\n"
          ]
        }
      ],
      "source": [
        "#@title âœ… Setup: GPU, deps, optional Drive\n",
        "import sys, os, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "# Quiet installs\n",
        "!pip -q install torchvision torchaudio scikit-learn matplotlib\n",
        "\n",
        "# Optional: save to Drive\n",
        "USE_DRIVE = False  #@param {type:\"boolean\"}\n",
        "ARTIFACT_DIR = \"/content/drive/MyDrive/vgg19_cifar10_prune_artifacts\" if USE_DRIVE else \"/content/artifacts\"\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "print(\"Artifacts â†’\", ARTIFACT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ”§ Imports, seeds, metrics, dataloaders, plotting (fixed)\n",
        "import time, copy, random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, classification_report)\n",
        "from torch.nn.utils import prune\n",
        "\n",
        "# Repro (keep cudnn fast)\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "CIFAR10_CLASSES = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "def build_dataloaders(data_root, batch_size, num_workers=2, val_ratio=0.1):\n",
        "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                                     std=[0.2470, 0.2435, 0.2616])\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    train_full = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n",
        "    test_set   = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n",
        "\n",
        "    n_val = int(len(train_full) * val_ratio)\n",
        "    n_train = len(train_full) - n_val\n",
        "    train_set, val_set = random_split(train_full, [n_train, n_val],\n",
        "                                      generator=torch.Generator().manual_seed(123))\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# === Plotting (matplotlib; no seaborn, no custom colors) ===\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curves(history, outdir):\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"],   label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True, linestyle=\":\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.tight_layout(); plt.savefig(Path(outdir)/\"loss_curves.png\"); plt.close()\n",
        "    # Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"epoch\"], history[\"train_acc\"], label=\"train_acc\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_acc\"],   label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True, linestyle=\":\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.tight_layout(); plt.savefig(Path(outdir)/\"acc_curves.png\"); plt.close()\n",
        "\n",
        "def plot_confusion(cm, classes, outpath, normalize=False, title=\"Confusion Matrix\"):\n",
        "    # FIX: dynamic formatting; ints for raw CM, .2f for normalized\n",
        "    cm = np.array(cm)\n",
        "    if normalize:\n",
        "        cm = cm.astype(np.float64)\n",
        "        with np.errstate(all='ignore'):\n",
        "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
        "            cm = np.nan_to_num(cm)\n",
        "        fmt = \".2f\"\n",
        "    else:\n",
        "        if cm.dtype.kind == 'f':\n",
        "            cm = np.rint(cm).astype(np.int64)\n",
        "        fmt = \"d\"\n",
        "\n",
        "    plt.figure(figsize=(6.5, 5.5))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title); plt.colorbar()\n",
        "    ticks = np.arange(len(classes))\n",
        "    plt.xticks(ticks, classes, rotation=45, ha=\"right\")\n",
        "    plt.yticks(ticks, classes)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\")\n",
        "    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\")\n",
        "    Path(outpath).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.tight_layout(); plt.savefig(outpath); plt.close()\n",
        "\n",
        "def plot_per_class_bars(report_dict, outstem):\n",
        "    y_true, y_pred = report_dict.get(\"y_true\"), report_dict.get(\"y_pred\")\n",
        "    if y_true is None or y_pred is None: return\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(10)), zero_division=0)\n",
        "    x = np.arange(len(CIFAR10_CLASSES))\n",
        "    # Precision\n",
        "    plt.figure(); plt.bar(x, p); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Precision\"); plt.title(\"Per-class Precision\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_precision.png\"); plt.close()\n",
        "    # Recall\n",
        "    plt.figure(); plt.bar(x, r); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Recall\"); plt.title(\"Per-class Recall\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_recall.png\"); plt.close()\n",
        "    # F1\n",
        "    plt.figure(); plt.bar(x, f1); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"F1-score\"); plt.title(\"Per-class F1-score\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_f1.png\"); plt.close()\n"
      ],
      "metadata": {
        "id": "tdsFH-X18tCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸ§  Model (VGG19), train/eval, structured pruning, sparsity\n",
        "def build_vgg19(num_classes=10, pretrained=False):\n",
        "    model = models.vgg19(weights=models.VGG19_Weights.DEFAULT if pretrained else None)\n",
        "    in_features = model.classifier[-1].in_features\n",
        "    head = list(model.classifier.children())\n",
        "    head[-1] = nn.Linear(in_features, num_classes)\n",
        "    model.classifier = nn.Sequential(*head)\n",
        "    return model\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler:\n",
        "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
        "                logits = model(x); loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
        "        else:\n",
        "            logits = model(x); loss = criterion(logits, y)\n",
        "            loss.backward(); optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, return_preds=False):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    total, running_loss = 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x); loss = criterion(logits, y)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        y_true.append(y.cpu().numpy()); y_pred.append(pred.cpu().numpy())\n",
        "        total += y.size(0)\n",
        "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p_mac, r_mac, f1_mac, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "    p_mic, r_mic, f1_mic, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
        "    return {\n",
        "        \"loss\": running_loss / total,\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": p_mac, \"recall_macro\": r_mac, \"f1_macro\": f1_mac,\n",
        "        \"precision_micro\": p_mic, \"recall_micro\": r_mic, \"f1_micro\": f1_mic,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"y_true\": y_true if return_preds else None,\n",
        "        \"y_pred\": y_pred if return_preds else None,\n",
        "        \"class_report\": classification_report(y_true, y_pred, target_names=CIFAR10_CLASSES, zero_division=0)\n",
        "    }\n",
        "\n",
        "def prune_for_inference(model, amount=0.3, structured=True, progressive=True):\n",
        "    \"\"\"\n",
        "    Structured pruning by default.\n",
        "      - Conv2d: prune output channels (dim=0) by L2 norm\n",
        "      - Linear: prune output units (dim=0)\n",
        "    progressive=True â†’ slightly more pruning in deeper layers.\n",
        "    Note: shapes are not physically shrunk; channels are zeroed (structured sparsity).\n",
        "    \"\"\"\n",
        "    m = copy.deepcopy(model)\n",
        "    # collect target layers\n",
        "    targets = [mod for mod in m.modules() if isinstance(mod, (nn.Conv2d, nn.Linear))]\n",
        "    if not targets:\n",
        "        return m\n",
        "\n",
        "    if structured:\n",
        "        if progressive and len(targets) > 1:\n",
        "            import torch as _torch\n",
        "            scales = _torch.linspace(0.8, 1.2, steps=len(targets)).tolist()\n",
        "            per_layer = [max(0.0, min(0.95, s * amount)) for s in scales]\n",
        "        else:\n",
        "            per_layer = [max(0.0, min(0.95, amount))] * len(targets)\n",
        "\n",
        "        for mod, amt in zip(targets, per_layer):\n",
        "            if isinstance(mod, nn.Conv2d):\n",
        "                prune.ln_structured(mod, name=\"weight\", amount=amt, n=2, dim=0)  # drop output channels\n",
        "                prune.remove(mod, \"weight\")\n",
        "            elif isinstance(mod, nn.Linear):\n",
        "                prune.ln_structured(mod, name=\"weight\", amount=amt, n=2, dim=0)  # drop output units\n",
        "                prune.remove(mod, \"weight\")\n",
        "    else:\n",
        "        for mod in targets:\n",
        "            prune.l1_unstructured(mod, name=\"weight\", amount=amount)\n",
        "            prune.remove(mod, \"weight\")\n",
        "    return m\n",
        "\n",
        "@torch.no_grad()\n",
        "def report_sparsity(model):\n",
        "    zeros_total, params_total = 0, 0\n",
        "    details = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if p is None or p.numel() == 0:\n",
        "            continue\n",
        "        num = p.numel()\n",
        "        z = (p == 0).sum().item()\n",
        "        frac = z / num\n",
        "        zeros_total += z; params_total += num\n",
        "        details.append((name, z, num, frac))\n",
        "    global_frac = zeros_total / params_total if params_total > 0 else 0.0\n",
        "    return global_frac, details\n"
      ],
      "metadata": {
        "id": "Fi2xuRUS8yXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ðŸš€ Train, structured prune-at-inference, evaluate, plot, save (self-contained)\n",
        "\n",
        "# --- knobs ---\n",
        "SEED = 42                 #@param {type:\"integer\"}\n",
        "EPOCHS = 40               #@param {type:\"integer\"}\n",
        "BATCH  = 256              #@param {type:\"integer\"}\n",
        "LR     = 0.01             #@param {type:\"number\"}\n",
        "MOMENTUM = 0.9            #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4       #@param {type:\"number\"}\n",
        "STEP_SIZE = 15            #@param {type:\"integer\"}\n",
        "GAMMA     = 0.1           #@param {type:\"number\"}\n",
        "\n",
        "# --- pruning ---\n",
        "STRUCTURED   = True       # default: structured pruning\n",
        "PRUNE_AMOUNT = 0.30       # try 0.2â€“0.5; >0.3 usually needs FT\n",
        "\n",
        "# --- plotting & FT ---\n",
        "PLOT = True               # save curves + CMs + per-class bars\n",
        "DO_FT = True              # short fine-tune after pruning (recommended for structured)\n",
        "FINE_TUNE_EPOCHS = 5      # small bump to recover accuracy\n",
        "FINE_TUNE_LR     = 5e-4\n",
        "\n",
        "# ---------- imports you already had ----------\n",
        "import time, copy, random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                             confusion_matrix, classification_report)\n",
        "from torch.nn.utils import prune\n",
        "\n",
        "# CIFAR-10 class names (used by plots)\n",
        "CIFAR10_CLASSES = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "# --- tiny utils we rely on here ---\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "def build_dataloaders(data_root, batch_size, num_workers=2, val_ratio=0.1):\n",
        "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                                     std=[0.2470, 0.2435, 0.2616])\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "    train_full = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n",
        "    test_set   = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n",
        "\n",
        "    n_val = int(len(train_full) * val_ratio)\n",
        "    n_train = len(train_full) - n_val\n",
        "    train_set, val_set = random_split(train_full, [n_train, n_val],\n",
        "                                      generator=torch.Generator().manual_seed(123))\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def build_vgg19(num_classes=10, pretrained=False):\n",
        "    model = models.vgg19(weights=models.VGG19_Weights.DEFAULT if pretrained else None)\n",
        "    in_features = model.classifier[-1].in_features\n",
        "    head = list(model.classifier.children())\n",
        "    head[-1] = nn.Linear(in_features, num_classes)\n",
        "    model.classifier = nn.Sequential(*head)\n",
        "    return model\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler:\n",
        "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, return_preds=False):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    total, running_loss = 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        y_true.append(y.cpu().numpy())\n",
        "        y_pred.append(pred.cpu().numpy())\n",
        "        total += y.size(0)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p_mac, r_mac, f1_mac, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "    p_mic, r_mic, f1_mic, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(10)))\n",
        "    return {\n",
        "        \"loss\": running_loss / total,\n",
        "        \"accuracy\": acc,\n",
        "        \"precision_macro\": p_mac, \"recall_macro\": r_mac, \"f1_macro\": f1_mac,\n",
        "        \"precision_micro\": p_mic, \"recall_micro\": r_mic, \"f1_micro\": f1_mic,\n",
        "        \"confusion_matrix\": cm,\n",
        "        \"y_true\": y_true if return_preds else None,\n",
        "        \"y_pred\": y_pred if return_preds else None,\n",
        "        \"class_report\": classification_report(y_true, y_pred, target_names=CIFAR10_CLASSES, zero_division=0)\n",
        "    }\n",
        "\n",
        "def prune_for_inference(model, amount=0.3, structured=True):\n",
        "    \"\"\"\n",
        "    Structured pruning (default):\n",
        "      - Conv2d: prune output channels (dim=0) by L2 norm\n",
        "      - Linear: prune output units (dim=0)\n",
        "    Leaves shapes the same (structured sparsity).\n",
        "    \"\"\"\n",
        "    m = copy.deepcopy(model)\n",
        "    if structured:\n",
        "        for mod in m.modules():\n",
        "            if isinstance(mod, nn.Conv2d):\n",
        "                prune.ln_structured(mod, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "                prune.remove(mod, \"weight\")\n",
        "            elif isinstance(mod, nn.Linear):\n",
        "                prune.ln_structured(mod, name=\"weight\", amount=amount, n=2, dim=0)\n",
        "                prune.remove(mod, \"weight\")\n",
        "    else:\n",
        "        for mod in m.modules():\n",
        "            if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
        "                prune.l1_unstructured(mod, name=\"weight\", amount=amount)\n",
        "                prune.remove(mod, \"weight\")\n",
        "    return m\n",
        "\n",
        "@torch.no_grad()\n",
        "def report_sparsity(model):\n",
        "    zeros_total, params_total = 0, 0\n",
        "    details = []\n",
        "    for name, p in model.named_parameters():\n",
        "        if p is None or p.numel() == 0:\n",
        "            continue\n",
        "        num = p.numel()\n",
        "        z = (p == 0).sum().item()\n",
        "        frac = z / num\n",
        "        zeros_total += z; params_total += num\n",
        "        details.append((name, z, num, frac))\n",
        "    global_frac = zeros_total / params_total if params_total > 0 else 0.0\n",
        "    return global_frac, details\n",
        "\n",
        "# --- plotting helpers embedded here (fixed confusion-matrix formatting) ---\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_curves(history, outdir):\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"],   label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.grid(True, linestyle=\":\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.tight_layout(); plt.savefig(Path(outdir)/\"loss_curves.png\"); plt.close()\n",
        "    # Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"epoch\"], history[\"train_acc\"], label=\"train_acc\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_acc\"],   label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.grid(True, linestyle=\":\")\n",
        "    plt.title(\"Training vs Validation Accuracy\")\n",
        "    plt.tight_layout(); plt.savefig(Path(outdir)/\"acc_curves.png\"); plt.close()\n",
        "\n",
        "def plot_confusion(cm, classes, outpath, normalize=False, title=\"Confusion Matrix\"):\n",
        "    cm = np.array(cm)\n",
        "    if normalize:\n",
        "        cm = cm.astype(np.float64)\n",
        "        with np.errstate(all='ignore'):\n",
        "            cm = cm / cm.sum(axis=1, keepdims=True)\n",
        "            cm = np.nan_to_num(cm)\n",
        "        fmt = \".2f\"\n",
        "    else:\n",
        "        if cm.dtype.kind == 'f':\n",
        "            cm = np.rint(cm).astype(np.int64)\n",
        "        fmt = \"d\"\n",
        "\n",
        "    plt.figure(figsize=(6.5, 5.5))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title(title); plt.colorbar()\n",
        "    ticks = np.arange(len(classes))\n",
        "    plt.xticks(ticks, classes, rotation=45, ha=\"right\")\n",
        "    plt.yticks(ticks, classes)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], fmt), ha=\"center\", va=\"center\")\n",
        "    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\")\n",
        "    Path(outpath).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.tight_layout(); plt.savefig(outpath); plt.close()\n",
        "\n",
        "def plot_per_class_bars(report_dict, outstem):\n",
        "    y_true, y_pred = report_dict.get(\"y_true\"), report_dict.get(\"y_pred\")\n",
        "    if y_true is None or y_pred is None: return\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=list(range(10)), zero_division=0)\n",
        "    x = np.arange(len(CIFAR10_CLASSES))\n",
        "    # Precision\n",
        "    plt.figure(); plt.bar(x, p); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Precision\"); plt.title(\"Per-class Precision\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_precision.png\"); plt.close()\n",
        "    # Recall\n",
        "    plt.figure(); plt.bar(x, r); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"Recall\"); plt.title(\"Per-class Recall\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_recall.png\"); plt.close()\n",
        "    # F1\n",
        "    plt.figure(); plt.bar(x, f1); plt.xticks(x, CIFAR10_CLASSES, rotation=45, ha=\"right\")\n",
        "    plt.ylabel(\"F1-score\"); plt.title(\"Per-class F1-score\"); plt.tight_layout()\n",
        "    plt.savefig(f\"{outstem}_f1.png\"); plt.close()\n",
        "\n",
        "# ---------------- run ----------------\n",
        "seed_everything(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "train_loader, val_loader, test_loader = build_dataloaders(\"/content/data\", BATCH)\n",
        "\n",
        "model = build_vgg19(num_classes=10, pretrained=False).to(device)\n",
        "tot, trn = count_params(model)\n",
        "print(f\"VGG19 params: total={tot/1e6:.2f}M, trainable={trn/1e6:.2f}M\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"lr\": []}\n",
        "best_val_f1 = -1.0\n",
        "best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print(\"\\n=== TRAINING ===\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
        "    val_rep = evaluate(model, val_loader, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"train_loss\"].append(tr_loss)\n",
        "    history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(val_rep[\"loss\"])\n",
        "    history[\"val_acc\"].append(val_rep[\"accuracy\"])\n",
        "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    if val_rep[\"f1_macro\"] > best_val_f1:\n",
        "        best_val_f1 = val_rep[\"f1_macro\"]\n",
        "        best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    print(f\"Epoch {epoch:02d}/{EPOCHS} | tr_loss {tr_loss:.4f} tr_acc {tr_acc:.4f} \"\n",
        "          f\"| val_loss {val_rep['loss']:.4f} val_acc {val_rep['accuracy']:.4f} \"\n",
        "          f\"| val_f1(macro) {val_rep['f1_macro']:.4f} | {dt:.1f}s\")\n",
        "\n",
        "# Load best weights by validation macro-F1\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "# Baseline test\n",
        "print(\"\\n=== TEST: Baseline (no pruning) ===\")\n",
        "base_rep = evaluate(model, test_loader, device, return_preds=True)\n",
        "print(base_rep[\"class_report\"])\n",
        "print(f\"Acc: {base_rep['accuracy']:.4f} | F1(macro): {base_rep['f1_macro']:.4f}\")\n",
        "\n",
        "# Structured prune for inference\n",
        "print(\"\\n=== APPLY STRUCTURED PRUNING (inference) ===\")\n",
        "pruned_model = prune_for_inference(model, amount=PRUNE_AMOUNT, structured=STRUCTURED).to(device)\n",
        "tot_p, trn_p = count_params(pruned_model)\n",
        "print(f\"Pruned params: total={tot_p/1e6:.2f}M, trainable={trn_p/1e6:.2f}M \"\n",
        "      f\"(amount={PRUNE_AMOUNT}, structured={STRUCTURED})\")\n",
        "\n",
        "# Sparsity report\n",
        "global_sparsity, details = report_sparsity(pruned_model)\n",
        "print(f\"Global weight sparsity after pruning: {100*global_sparsity:.2f}%\")\n",
        "topk = sorted(details, key=lambda x: x[3], reverse=True)[:5]\n",
        "print(\"Top sparse params:\")\n",
        "for name, z, n, frac in topk:\n",
        "    print(f\"  {name:45s} {100*frac:5.1f}% zeros  ({z}/{n})\")\n",
        "\n",
        "# Pruned test\n",
        "print(\"\\n=== TEST: Pruned (structured) ===\")\n",
        "pruned_rep = evaluate(pruned_model, test_loader, device, return_preds=True)\n",
        "print(pruned_rep[\"class_report\"])\n",
        "print(f\"Acc: {pruned_rep['accuracy']:.4f} | F1(macro): {pruned_rep['f1_macro']:.4f}\")\n",
        "\n",
        "# Optional short fine-tune to recover accuracy\n",
        "if DO_FT:\n",
        "    print(\"\\n=== FINE-TUNE (structured pruned model) ===\")\n",
        "    ft_model = copy.deepcopy(pruned_model).to(device)\n",
        "    optimizer_ft = optim.SGD(ft_model.parameters(), lr=FINE_TUNE_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler_ft = StepLR(optimizer_ft, step_size=max(1, FINE_TUNE_EPOCHS//2), gamma=0.5)\n",
        "    for e in range(1, FINE_TUNE_EPOCHS+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(ft_model, train_loader, criterion, optimizer_ft, device)\n",
        "        val_rep = evaluate(ft_model, val_loader, device)\n",
        "        scheduler_ft.step()\n",
        "        print(f\"FT {e:02d}/{FINE_TUNE_EPOCHS} | tr_loss {tr_loss:.4f} tr_acc {tr_acc:.4f} \"\n",
        "              f\"| val_loss {val_rep['loss']:.4f} val_acc {val_rep['accuracy']:.4f} \"\n",
        "              f\"| val_f1(macro) {val_rep['f1_macro']:.4f}\")\n",
        "\n",
        "    print(\"\\n=== TEST: Pruned + Fine-Tuned ===\")\n",
        "    ft_rep = evaluate(ft_model, test_loader, device, return_preds=True)\n",
        "    print(ft_rep[\"class_report\"])\n",
        "    print(f\"Acc: {ft_rep['accuracy']:.4f} | F1(macro): {ft_rep['f1_macro']:.4f}\")\n",
        "\n",
        "# Comparison table\n",
        "def row(name, rep):\n",
        "    return [name, f\"{rep['accuracy']:.4f}\", f\"{rep['precision_macro']:.4f}\", f\"{rep['recall_macro']:.4f}\", f\"{rep['f1_macro']:.4f}\"]\n",
        "header = [\"Model\", \"Acc\", \"Prec(mac)\", \"Rec(mac)\", \"F1(mac)\"]\n",
        "rows = [header, row(\"Baseline\", base_rep), row(\"Pruned\", pruned_rep)]\n",
        "if DO_FT:\n",
        "    rows.append(row(\"Pruned+FT\", ft_rep))\n",
        "colw = [max(len(r[i]) for r in rows) for i in range(len(header))]\n",
        "print(\"\\n=== COMPARISON (test) ===\")\n",
        "print(\" | \".join(h.ljust(colw[i]) for i, h in enumerate(header)))\n",
        "print(\"-+-\".join(\"-\"*colw[i] for i in range(len(header))))\n",
        "for r in rows[1:]:\n",
        "    print(\" | \".join(r[i].ljust(colw[i]) for i in range(len(header))))\n",
        "\n",
        "# Save artifacts\n",
        "ARTIFACT_DIR = \"/content/artifacts\"\n",
        "outdir = Path(ARTIFACT_DIR); outdir.mkdir(exist_ok=True, parents=True)\n",
        "torch.save(model.state_dict(), outdir/\"vgg19_cifar10_baseline.pt\")\n",
        "torch.save(pruned_model.state_dict(), outdir/\"vgg19_cifar10_pruned.pt\")\n",
        "if DO_FT:\n",
        "    torch.save(ft_model.state_dict(), outdir/\"vgg19_cifar10_pruned_finetuned.pt\")\n",
        "np.save(outdir/\"history.npy\", history, allow_pickle=True)\n",
        "\n",
        "# Plots\n",
        "if PLOT:\n",
        "    plot_curves(history, outdir)\n",
        "    # Baseline\n",
        "    plot_confusion(base_rep[\"confusion_matrix\"], CIFAR10_CLASSES, outdir/\"cm_baseline_raw.png\",\n",
        "                   normalize=False, title=\"Confusion Matrix (Baseline)\")\n",
        "    plot_confusion(base_rep[\"confusion_matrix\"], CIFAR10_CLASSES, outdir/\"cm_baseline_norm.png\",\n",
        "                   normalize=True, title=\"Normalized Confusion Matrix (Baseline)\")\n",
        "    plot_per_class_bars(base_rep, str(outdir/\"baseline_perclass\"))\n",
        "    # Pruned\n",
        "    plot_confusion(pruned_rep[\"confusion_matrix\"], CIFAR10_CLASSES, outdir/\"cm_pruned_raw.png\",\n",
        "                   normalize=False, title=\"Confusion Matrix (Pruned)\")\n",
        "    plot_confusion(pruned_rep[\"confusion_matrix\"], CIFAR10_CLASSES, outdir/\"cm_pruned_norm.png\",\n",
        "                   normalize=True, title=\"Normalized Confusion Matrix (Pruned)\")\n",
        "    plot_per_class_bars(pruned_rep, str(outdir/\"pruned_perclass\"))\n",
        "    # Pruned + FT\n",
        "    if DO_FT:\n",
        "        plot_confusion(ft_rep[\"confusion_matrix\"], CIFAR10_CLASSES, outdir/\"cm_pruned_ft_norm.png\",\n",
        "                       normalize=True, title=\"Normalized Confusion Matrix (Pruned + FT)\")\n",
        "        plot_per_class_bars(ft_rep, str(outdir/\"pruned_ft_perclass\"))\n",
        "\n",
        "print(\"\\nArtifacts saved in:\", outdir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCGVtzRj82aX",
        "outputId": "9f52290f-8cb2-4717-fd8c-449406ab145e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:05<00:00, 29.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG19 params: total=139.61M, trainable=139.61M\n",
            "\n",
            "=== TRAINING ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2055885685.py:260: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/40 | tr_loss 2.2785 tr_acc 0.1222 | val_loss 2.1949 val_acc 0.1632 | val_f1(macro) 0.0911 | 53.5s\n",
            "Epoch 02/40 | tr_loss 2.1103 tr_acc 0.1932 | val_loss 2.1268 val_acc 0.1872 | val_f1(macro) 0.1162 | 19.9s\n",
            "Epoch 03/40 | tr_loss 1.9508 tr_acc 0.2492 | val_loss 1.8555 val_acc 0.2914 | val_f1(macro) 0.2541 | 19.9s\n",
            "Epoch 04/40 | tr_loss 1.8114 tr_acc 0.3148 | val_loss 1.7737 val_acc 0.3420 | val_f1(macro) 0.3257 | 20.0s\n",
            "Epoch 05/40 | tr_loss 1.6847 tr_acc 0.3730 | val_loss 1.5493 val_acc 0.4256 | val_f1(macro) 0.4064 | 19.8s\n",
            "Epoch 06/40 | tr_loss 1.5340 tr_acc 0.4381 | val_loss 1.4586 val_acc 0.4774 | val_f1(macro) 0.4716 | 20.5s\n",
            "Epoch 07/40 | tr_loss 1.3982 tr_acc 0.4917 | val_loss 1.3322 val_acc 0.5180 | val_f1(macro) 0.5049 | 19.9s\n",
            "Epoch 08/40 | tr_loss 1.2649 tr_acc 0.5438 | val_loss 1.2299 val_acc 0.5602 | val_f1(macro) 0.5561 | 20.2s\n",
            "Epoch 09/40 | tr_loss 1.1888 tr_acc 0.5725 | val_loss 1.2380 val_acc 0.5622 | val_f1(macro) 0.5522 | 20.3s\n",
            "Epoch 10/40 | tr_loss 1.1331 tr_acc 0.5944 | val_loss 1.1181 val_acc 0.6006 | val_f1(macro) 0.5957 | 19.8s\n",
            "Epoch 11/40 | tr_loss 1.0898 tr_acc 0.6153 | val_loss 1.0565 val_acc 0.6228 | val_f1(macro) 0.6218 | 20.2s\n",
            "Epoch 12/40 | tr_loss 1.0109 tr_acc 0.6441 | val_loss 1.0734 val_acc 0.6202 | val_f1(macro) 0.6177 | 20.1s\n",
            "Epoch 13/40 | tr_loss 0.9809 tr_acc 0.6580 | val_loss 0.9512 val_acc 0.6736 | val_f1(macro) 0.6727 | 20.2s\n",
            "Epoch 14/40 | tr_loss 0.9428 tr_acc 0.6737 | val_loss 0.9027 val_acc 0.6846 | val_f1(macro) 0.6847 | 20.8s\n",
            "Epoch 15/40 | tr_loss 0.8999 tr_acc 0.6884 | val_loss 0.9148 val_acc 0.6866 | val_f1(macro) 0.6824 | 21.2s\n",
            "Epoch 16/40 | tr_loss 0.7577 tr_acc 0.7362 | val_loss 0.7787 val_acc 0.7234 | val_f1(macro) 0.7233 | 20.0s\n",
            "Epoch 17/40 | tr_loss 0.7256 tr_acc 0.7489 | val_loss 0.7722 val_acc 0.7300 | val_f1(macro) 0.7315 | 20.5s\n",
            "Epoch 18/40 | tr_loss 0.7202 tr_acc 0.7511 | val_loss 0.7724 val_acc 0.7268 | val_f1(macro) 0.7263 | 20.4s\n",
            "Epoch 19/40 | tr_loss 0.7086 tr_acc 0.7541 | val_loss 0.7795 val_acc 0.7264 | val_f1(macro) 0.7273 | 20.2s\n",
            "Epoch 20/40 | tr_loss 0.7050 tr_acc 0.7566 | val_loss 0.7641 val_acc 0.7352 | val_f1(macro) 0.7343 | 20.5s\n",
            "Epoch 21/40 | tr_loss 0.6995 tr_acc 0.7578 | val_loss 0.7513 val_acc 0.7376 | val_f1(macro) 0.7384 | 19.9s\n",
            "Epoch 22/40 | tr_loss 0.6783 tr_acc 0.7657 | val_loss 0.7663 val_acc 0.7334 | val_f1(macro) 0.7355 | 20.4s\n",
            "Epoch 23/40 | tr_loss 0.6817 tr_acc 0.7627 | val_loss 0.7590 val_acc 0.7380 | val_f1(macro) 0.7392 | 20.0s\n",
            "Epoch 24/40 | tr_loss 0.6776 tr_acc 0.7649 | val_loss 0.7515 val_acc 0.7336 | val_f1(macro) 0.7344 | 19.9s\n",
            "Epoch 25/40 | tr_loss 0.6668 tr_acc 0.7678 | val_loss 0.7498 val_acc 0.7410 | val_f1(macro) 0.7381 | 20.1s\n",
            "Epoch 26/40 | tr_loss 0.6684 tr_acc 0.7686 | val_loss 0.7156 val_acc 0.7470 | val_f1(macro) 0.7487 | 19.9s\n",
            "Epoch 27/40 | tr_loss 0.6594 tr_acc 0.7732 | val_loss 0.7253 val_acc 0.7470 | val_f1(macro) 0.7459 | 20.5s\n",
            "Epoch 28/40 | tr_loss 0.6571 tr_acc 0.7740 | val_loss 0.7262 val_acc 0.7506 | val_f1(macro) 0.7521 | 20.2s\n",
            "Epoch 29/40 | tr_loss 0.6467 tr_acc 0.7756 | val_loss 0.7314 val_acc 0.7502 | val_f1(macro) 0.7495 | 20.0s\n",
            "Epoch 30/40 | tr_loss 0.6442 tr_acc 0.7768 | val_loss 0.7389 val_acc 0.7394 | val_f1(macro) 0.7395 | 20.0s\n",
            "Epoch 31/40 | tr_loss 0.6246 tr_acc 0.7828 | val_loss 0.6925 val_acc 0.7578 | val_f1(macro) 0.7578 | 20.4s\n",
            "Epoch 32/40 | tr_loss 0.6171 tr_acc 0.7864 | val_loss 0.6934 val_acc 0.7646 | val_f1(macro) 0.7639 | 20.5s\n",
            "Epoch 33/40 | tr_loss 0.6198 tr_acc 0.7848 | val_loss 0.7240 val_acc 0.7526 | val_f1(macro) 0.7529 | 19.9s\n",
            "Epoch 34/40 | tr_loss 0.6216 tr_acc 0.7849 | val_loss 0.7068 val_acc 0.7580 | val_f1(macro) 0.7579 | 19.9s\n",
            "Epoch 35/40 | tr_loss 0.6147 tr_acc 0.7867 | val_loss 0.7127 val_acc 0.7512 | val_f1(macro) 0.7506 | 19.9s\n",
            "Epoch 36/40 | tr_loss 0.6142 tr_acc 0.7871 | val_loss 0.6947 val_acc 0.7650 | val_f1(macro) 0.7659 | 20.3s\n",
            "Epoch 37/40 | tr_loss 0.6177 tr_acc 0.7867 | val_loss 0.6866 val_acc 0.7634 | val_f1(macro) 0.7641 | 20.2s\n",
            "Epoch 38/40 | tr_loss 0.6129 tr_acc 0.7863 | val_loss 0.7020 val_acc 0.7532 | val_f1(macro) 0.7538 | 20.0s\n",
            "Epoch 39/40 | tr_loss 0.6130 tr_acc 0.7883 | val_loss 0.7026 val_acc 0.7546 | val_f1(macro) 0.7553 | 20.4s\n",
            "Epoch 40/40 | tr_loss 0.6086 tr_acc 0.7890 | val_loss 0.6949 val_acc 0.7608 | val_f1(macro) 0.7608 | 20.0s\n",
            "\n",
            "=== TEST: Baseline (no pruning) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.84      0.84      0.84      1000\n",
            "  automobile       0.93      0.94      0.94      1000\n",
            "        bird       0.77      0.74      0.76      1000\n",
            "         cat       0.69      0.62      0.65      1000\n",
            "        deer       0.78      0.85      0.81      1000\n",
            "         dog       0.73      0.76      0.74      1000\n",
            "        frog       0.84      0.89      0.87      1000\n",
            "       horse       0.90      0.85      0.87      1000\n",
            "        ship       0.91      0.91      0.91      1000\n",
            "       truck       0.91      0.91      0.91      1000\n",
            "\n",
            "    accuracy                           0.83     10000\n",
            "   macro avg       0.83      0.83      0.83     10000\n",
            "weighted avg       0.83      0.83      0.83     10000\n",
            "\n",
            "Acc: 0.8311 | F1(macro): 0.8302\n",
            "\n",
            "=== APPLY STRUCTURED PRUNING (inference) ===\n",
            "Pruned params: total=139.61M, trainable=139.61M (amount=0.3, structured=True)\n",
            "Global weight sparsity after pruning: 30.01%\n",
            "Top sparse params:\n",
            "  features.10.weight                             30.1% zeros  (88704/294912)\n",
            "  features.12.weight                             30.1% zeros  (177408/589824)\n",
            "  features.14.weight                             30.1% zeros  (177408/589824)\n",
            "  features.16.weight                             30.1% zeros  (177408/589824)\n",
            "  features.19.weight                             30.1% zeros  (354816/1179648)\n",
            "\n",
            "=== TEST: Pruned (structured) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.00      0.00      0.00      1000\n",
            "  automobile       1.00      0.02      0.04      1000\n",
            "        bird       0.00      0.00      0.00      1000\n",
            "         cat       0.00      0.00      0.00      1000\n",
            "        deer       0.12      0.99      0.21      1000\n",
            "         dog       0.00      0.00      0.00      1000\n",
            "        frog       0.29      0.48      0.37      1000\n",
            "       horse       0.00      0.00      0.00      1000\n",
            "        ship       1.00      0.01      0.01      1000\n",
            "       truck       1.00      0.04      0.07      1000\n",
            "\n",
            "    accuracy                           0.15     10000\n",
            "   macro avg       0.34      0.15      0.07     10000\n",
            "weighted avg       0.34      0.15      0.07     10000\n",
            "\n",
            "Acc: 0.1541 | F1(macro): 0.0702\n",
            "\n",
            "=== FINE-TUNE (structured pruned model) ===\n",
            "FT 01/5 | tr_loss 1.3893 tr_acc 0.5441 | val_loss 1.0087 val_acc 0.6542 | val_f1(macro) 0.6313\n",
            "FT 02/5 | tr_loss 0.9105 tr_acc 0.6862 | val_loss 0.8813 val_acc 0.6914 | val_f1(macro) 0.6858\n",
            "FT 03/5 | tr_loss 0.8302 tr_acc 0.7182 | val_loss 0.8548 val_acc 0.7028 | val_f1(macro) 0.6974\n",
            "FT 04/5 | tr_loss 0.8007 tr_acc 0.7248 | val_loss 0.8328 val_acc 0.7186 | val_f1(macro) 0.7184\n",
            "FT 05/5 | tr_loss 0.7756 tr_acc 0.7326 | val_loss 0.8220 val_acc 0.7142 | val_f1(macro) 0.7131\n",
            "\n",
            "=== TEST: Pruned + Fine-Tuned ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    airplane       0.83      0.80      0.82      1000\n",
            "  automobile       0.92      0.92      0.92      1000\n",
            "        bird       0.71      0.67      0.69      1000\n",
            "         cat       0.68      0.50      0.58      1000\n",
            "        deer       0.75      0.82      0.78      1000\n",
            "         dog       0.67      0.73      0.70      1000\n",
            "        frog       0.76      0.90      0.82      1000\n",
            "       horse       0.86      0.83      0.84      1000\n",
            "        ship       0.90      0.89      0.89      1000\n",
            "       truck       0.88      0.91      0.89      1000\n",
            "\n",
            "    accuracy                           0.80     10000\n",
            "   macro avg       0.79      0.80      0.79     10000\n",
            "weighted avg       0.79      0.80      0.79     10000\n",
            "\n",
            "Acc: 0.7958 | F1(macro): 0.7928\n",
            "\n",
            "=== COMPARISON (test) ===\n",
            "Model     | Acc    | Prec(mac) | Rec(mac) | F1(mac)\n",
            "----------+--------+-----------+----------+--------\n",
            "Baseline  | 0.8311 | 0.8304    | 0.8311   | 0.8302 \n",
            "Pruned    | 0.1541 | 0.3413    | 0.1541   | 0.0702 \n",
            "Pruned+FT | 0.7958 | 0.7948    | 0.7958   | 0.7928 \n",
            "\n",
            "Artifacts saved in: /content/artifacts\n"
          ]
        }
      ]
    }
  ]
}